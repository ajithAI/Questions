
# Questions

### Course 2 : Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization

### Week 1 : 
1. Explain Bias & Varience.
2. Explain Bias & Varience wrt to Train Error & Dev Error with example.
3. How to solve High Bias Problem ?
4. How to solve High Varience Problem ?
5. What are L1 & L2 Reguralizers. Explain mathematics. Which is better & why ?
6. What is Weight Decay ?
7. How L1 & L2 Reguralizers helps decrease varience ?
8. Explain the working of Dropout. How it behave on Test set ?
9. List all Regularization Techniques.
10. Explain Vanishing & Exploding Gradients problem.
11. How Xiavier Random Initilaization helps in Vanishing/Exploding Problem.


### Week 2 : 
1. Explain Batch vs Mini-Batch.
2. If Mini-Batch Size = 1, then the Gradient descent algorithm is called ?
3. Explain the concept of Exponentially Weighted Averages & Bias Correction.
4. Explain Gradient Descent with Momentum Optimization algorithm.
5. Explain RMSProp Optimization algorithm with Intuiion behind. ( b vs w ) 
6. Explain ADAM OPTIMIZER with Matchematics.
7. Explain Learning Rate Decay with various implementation formulas.
8. Explain Local Optima & Saddle point


### Week 3 : 
1. Explain how to choose Hyper parameters.
2. Explain about Batch Normalization with Formula. Explain why we dont need Bias parameters if we use BN ?
3. How co-varience shift issue can be solved with BN.
4. Explain working of BN at Test Time.
5. Explain Loss Function when the classifier is Softmax classifier.
